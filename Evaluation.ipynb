{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Pretrained Model (NSP and MLM tasks for BERT)"
      ],
      "metadata": {
        "id": "D_SSx8dRDe57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "alCz3xfcQJnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import random"
      ],
      "metadata": {
        "id": "40UCI02iDlYZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ],
      "metadata": {
        "id": "OUVFaaHSHBBd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_cleaned = datasets.filter(lambda example: len(example['text'])  > 0 and not example[\"text\"].startswith(\" =\"))"
      ],
      "metadata": {
        "id": "-9quFW2IHEfw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Dhairya/nlp-bert-wordWeavers-tokenizer\")"
      ],
      "metadata": {
        "id": "gPiN1weKHPxH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(example):\n",
        "    return tokenizer(example[\"text\"], add_special_tokens=False)\n",
        "\n",
        "datasets_tokenized = datasets_cleaned.map(\n",
        "    tokenization, batched=True, num_proc=4)"
      ],
      "metadata": {
        "id": "ZZKP92zsHcYD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_tokenized = datasets_tokenized.map(batched= True, remove_columns='text')"
      ],
      "metadata": {
        "id": "aCoxSXGuHuvK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenation(example):\n",
        "    BLOCK_SIZE= 255\n",
        "    # concatenating the inputs_ids, token_ids, attention_mask respectively to a list to create a single list of all the tokens\n",
        "    concatenated_examples = {}\n",
        "    for keys in example.keys(): # inputs_ids, token_ids, attention_mask\n",
        "        concatenated_examples[keys] = sum(example[keys], [])\n",
        "\n",
        "    # total length same across all the keys\n",
        "    n = len(concatenated_examples[list(example.keys())[0]])\n",
        "    n = (n//BLOCK_SIZE) * BLOCK_SIZE\n",
        "\n",
        "    # breaking the total combined list to get BLOCK SIZE chunks\n",
        "    result = {}\n",
        "    for keys, token_type in concatenated_examples.items():\n",
        "        result[keys] = []\n",
        "        for i in range(0, n, BLOCK_SIZE):\n",
        "            result[keys].append(token_type[i: i+BLOCK_SIZE])\n",
        "    return result\n",
        "\n",
        "datasets_block_size = datasets_tokenized.map(\n",
        "    concatenation, batched=True, batch_size=1000, num_proc=4)"
      ],
      "metadata": {
        "id": "33GRM9i9Hzsw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preparing_NSP_dataset(example, ind, dataset, n):\n",
        "    # NSP dataset is created such that 50% sentence 2 comes after sentence 1. Rest 50% it is random\n",
        "    sent_1 = example['input_ids']\n",
        "    attention_mask = [1] * 512\n",
        "    next_sentence_label = 1\n",
        "\n",
        "    if ind % 2 == 0:\n",
        "        next_ind = ind + 1\n",
        "        if next_ind < len(dataset['input_ids']):\n",
        "            sent_2 = dataset['input_ids'][next_ind]\n",
        "        else: # last sentence has no next sentence\n",
        "            next_ind = random.randint(0, n-1)\n",
        "            sent_2 = dataset['input_ids'][next_ind]\n",
        "            next_sentence_label = 0\n",
        "\n",
        "    else:\n",
        "        next_sentence_label = 0\n",
        "        next_ind = random.randint(0, n-1) # randomly choosing the next index\n",
        "        if next_ind == ind + 1: # if randomly choosed the next sentence then changing the next sentence label\n",
        "            next_sentence_label = 1\n",
        "        sent_2 = dataset['input_ids'][next_ind]\n",
        "\n",
        "    # input  =  [cls] + sent1 + [sep] + sent2\n",
        "    input_ids = [tokenizer.cls_token_id] + sent_1 + [tokenizer.sep_token_id] + sent_2\n",
        "    token_type_ids = [0] * (257) + [1] * (255)\n",
        "    attention_mask = [1] + example['attention_mask'] + [1] + dataset[next_ind]['attention_mask']\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'next_sentence_label': next_sentence_label\n",
        "    }"
      ],
      "metadata": {
        "id": "WMlUNQLdH2zp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation = datasets_block_size['validation']\n",
        "dataset_test = datasets_block_size['test']"
      ],
      "metadata": {
        "id": "e-1GfylYH5xr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_NSP_test = dataset_test.map(\n",
        "    lambda example, ind: preparing_NSP_dataset(example, ind, dataset_test, n =len(dataset_test)),with_indices=True, num_proc=32)"
      ],
      "metadata": {
        "id": "Dr_0D_tJIBhG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForPreTraining, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "FUIqHF27IPPc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = BertForPreTraining.from_pretrained(\"Dhairya/nlp-bert-wordWeavers-pretrained\")\n",
        "collater_pt = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15, return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "Dj5T1m01IRVR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define the Trainer and TrainingArguments\n",
        "trainer = Trainer(\n",
        "    model=pretrained_model,  # your pretrained model\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./results\",  # directory to save results\n",
        "        per_device_eval_batch_size=16,\n",
        "    ),\n",
        "    data_collator=collater_pt,  # your data collator\n",
        ")\n",
        "# Evaluate on the validation set (replace with your test dataset if needed)\n",
        "results = trainer.evaluate(eval_dataset=dataset_NSP_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "90ZPVQLRO1mr",
        "outputId": "b30f2a88-7d90-4c7e-8a61-203b6e656683"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='62' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [62/67 47:23 < 03:53, 0.02 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [67/67 50:40]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 8.028535842895508, 'eval_runtime': 3107.5866, 'eval_samples_per_second': 0.342, 'eval_steps_per_second': 0.022}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m6vWWMVTBN-",
        "outputId": "ebf6c8f3-d359-4fe9-d8b5-60c7906e5172"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 8.028535842895508, 'eval_runtime': 3107.5866, 'eval_samples_per_second': 0.342, 'eval_steps_per_second': 0.022}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "log_perplexity = results['eval_loss']\n",
        "perplexity = math.exp(log_perplexity)\n",
        "\n",
        "print(\"Log-Perplexity on Test Dataset: \", log_perplexity)\n",
        "print(\"Perplexity on Test Dataset: \", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEIqicobgTOx",
        "outputId": "eac645c5-0f72-42ca-e550-d4c72dba7a5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-Perplexity on Test Dataset:  8.028535842895508\n",
            "Perplexity on Test Dataset:  3067.247451804034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Finetuned Model (Classification on SST2 dataset)"
      ],
      "metadata": {
        "id": "T5iWpzuY7asI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "OIUn9o5mEIBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer, BertTokenizer, BertForSequenceClassification, EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "QJEinW-BQk17"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "sst2_dataset = load_dataset('glue', 'sst2')"
      ],
      "metadata": {
        "id": "ixfZW31lQpCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Dhairya/bert-wordWeavers-ft-sst2\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Dhairya/nlp-bert-wordWeavers-tokenizer\")"
      ],
      "metadata": {
        "id": "FSQwFLh7EVbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define a DataLoader for the test dataset\n",
        "batch_size = 32  # Adjust as needed\n",
        "test_dataloader = DataLoader(sst2_dataset['validation'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test dataset and make predictions\n",
        "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "    inputs = tokenizer(batch[\"sentence\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # Get predicted labels\n",
        "    preds = torch.argmax(logits, dim=1).tolist()\n",
        "    predicted_labels.extend(preds)\n",
        "\n",
        "    # Get true labels\n",
        "    true_labels.extend(batch[\"label\"].tolist())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrOIEKav97Qm",
        "outputId": "2414a49d-e65c-4569-e47f-294dfe65438f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 28/28 [02:15<00:00,  4.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5092\n",
            "Precision: 0.2593\n",
            "Recall: 0.5092\n",
            "F1 Score: 0.3436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWe2OKE2dZNR",
        "outputId": "0e385735-5d26-4763-85b2-58dfbb9c7b1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5092\n",
            "Precision: 0.2593\n",
            "Recall: 0.5092\n",
            "F1 Score: 0.3436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Finetuned Model (Question-Answering on SQuAD)"
      ],
      "metadata": {
        "id": "ZwCYHTLO7p0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering.from_pretrained(\"Dhairya/bert-wordweavers-ft-squad\")"
      ],
      "metadata": {
        "id": "GmqOBaGW8ixm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Dhairya/nlp-bert-wordweavers-tokenizer\")"
      ],
      "metadata": {
        "id": "zZZfQSmp8k-S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "squad = load_dataset(\"squad_v2\")"
      ],
      "metadata": {
        "id": "L70VnSBA8zXz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "complete_data = concatenate_datasets([squad['train'], squad['validation']])"
      ],
      "metadata": {
        "id": "GoXjj_9u81YK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_no_answers(example):\n",
        "    return bool(example['answers']['text'])\n",
        "def filter_no_question(example):\n",
        "    return bool(example['question'])\n",
        "# Filter out examples with no answers\n",
        "filtered_dataset = complete_data.filter(filter_no_answers)\n",
        "filtered_dataset = filtered_dataset.filter(filter_no_question)\n",
        "complete_data = filtered_dataset"
      ],
      "metadata": {
        "id": "8EAFwLKh83ni"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_test = complete_data.class_encode_column(\"title\").train_test_split(test_size=0.2, stratify_by_column=\"title\", seed = 1)"
      ],
      "metadata": {
        "id": "m2H0a0IR86BL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGQSSY8T88OB",
        "outputId": "365bb4d5-e949-423d-86b4-db740c55b80d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "metric_sq_v2 = load_metric('squad_v2')\n",
        "metric_bleu = load_metric('bleu')\n",
        "metric_meteor = load_metric('meteor')\n",
        "metric_rouge = load_metric('rouge')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vqT6_cC8-qt",
        "outputId": "377a5d8d-2087-4136-cc77-ae3adb6ee280"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-42d899239100>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric_sq_v2 = load_metric('squad_v2')\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\", model=model, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "yCD7InZo9BCa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvnS5pHBC_qm",
        "outputId": "1576e1bf-c587-44f3-ca4b-a66ec0746256"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will test for 400 samples from the test set. Set the MAX_EXAMPLES_TO_TEST variable to equal the num_testing_examples in order to test on the full test set (very time consuming)"
      ],
      "metadata": {
        "id": "SgoT-nLfp-NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "references = []\n",
        "predicted_answers = []\n",
        "reference_answers = []\n",
        "count = 0\n",
        "num_testing_examples = dataset_train_test['test'].num_rows\n",
        "\n",
        "MAX_EXAMPLES_TO_TEST = 400\n",
        "\n",
        "#Uncomment this variable to a smaller value like num_testing_examples/10 if you want to evaluate on a subset of the test set.\n",
        "# MAX_EXAMPLES_TO_TEST = num_testing_examples\n",
        "\n",
        "for example in dataset_train_test['test']:\n",
        "    count = count + 1\n",
        "    if(count>MAX_EXAMPLES_TO_TEST):\n",
        "      # print(count)\n",
        "      break\n",
        "    context = example[\"context\"]\n",
        "    question = example[\"question\"]\n",
        "    expected_answer_list = example['answers']['text']\n",
        "\n",
        "    if len(expected_answer_list) == 0:\n",
        "        expected_answer = ''\n",
        "    else:\n",
        "        expected_answer = expected_answer_list[0]\n",
        "    reference_answers.append(expected_answer)\n",
        "    predicted_answer = question_answerer(question=question, context=context)\n",
        "    predicted_answers.append(predicted_answer['answer'])\n",
        "    predictions.append({'prediction_text': predicted_answer, 'id': example['id'], 'no_answer_probability': 0})\n",
        "\n",
        "    # Append references in the required format\n",
        "    references.append({'answers': {'answer_start': [context.find(expected_answer)], 'text': [expected_answer]}, 'id': example['id']})\n",
        "    print(f\"Evaluating- {count}/{MAX_EXAMPLES_TO_TEST} complete\")"
      ],
      "metadata": {
        "id": "tcGzVi4n9MZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def find_met(ref, pred):\n",
        "  res = 0\n",
        "  for i in range(len(ref)):\n",
        "    res += nltk.translate.meteor_score.meteor_score([ref[i].split()], pred[i].split(), gamma=1)\n",
        "  res = res/len(ref)\n",
        "  return res"
      ],
      "metadata": {
        "id": "t_26e-MQ9RDK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0phbZzE9UDR",
        "outputId": "0c705e21-03c8-4347-c0e9-1cea36a2a188"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu([[ref.split()] for ref in reference_answers], [pred.split() for pred in predicted_answers])\n",
        "\n",
        "# Calculate METEOR score\n",
        "meteor_score = find_met(reference_answers, predicted_answers)\n",
        "\n",
        "predicted_answers = [\"  \" if not answer else answer for answer in predicted_answers]\n",
        "# Calculate ROUGE score\n",
        "rouge = Rouge()\n",
        "rouge_score = rouge.get_scores(predicted_answers, reference_answers, avg=True)\n",
        "\n",
        "# Calculate exact-match score\n",
        "exact_match = accuracy_score(reference_answers, predicted_answers)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1_score = 2 * (exact_match * bleu_score) / (exact_match + bleu_score)"
      ],
      "metadata": {
        "id": "jCsUaxT59clr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_sq_v2 = metric_sq_v2.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "yAsHDUSg9h0_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'F1 Score: {f1_score:.4f}')\n",
        "print(f'BLEU Score: {bleu_score:.4f}')\n",
        "print(f'Meteor Score: {meteor_score:.4f}')\n",
        "print(f'Exact Match Score: {exact_match:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8N1CVoG9nZo",
        "outputId": "acf8aa60-5126-4cd8-af21-ab44446a1957"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.0092\n",
            "BLEU Score: 0.0118\n",
            "Meteor Score: 0.0195\n",
            "Exact Match Score: 0.0075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rouge scores: \",rouge_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H6_baXD9pse",
        "outputId": "0e175574-fc15-4cb7-c1fe-fa1b4db7b7e0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge scores:  {'rouge-1': {'r': 0.09190275228998057, 'p': 0.04448469585969586, 'f': 0.04664135568935447}, 'rouge-2': {'r': 0.03417839105339105, 'p': 0.01077888777888778, 'f': 0.013060369158305038}, 'rouge-l': {'r': 0.09190275228998057, 'p': 0.04448469585969586, 'f': 0.04664135568935447}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SQuAD v2 scores: \", results_sq_v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onx9QiZk9rqp",
        "outputId": "efa1fb17-bc3e-411e-af15-6dad31c6b6b8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQuAD v2 scores:  {'exact': 0.0, 'f1': 2.7265609279686203, 'total': 400, 'HasAns_exact': 0.0, 'HasAns_f1': 2.7265609279686203, 'HasAns_total': 400, 'best_exact': 0.0, 'best_exact_thresh': 0.0, 'best_f1': 2.7265609279686203, 'best_f1_thresh': 0.0}\n"
          ]
        }
      ]
    }
  ]
}